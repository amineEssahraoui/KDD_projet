{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82b81b5d",
   "metadata": {},
   "source": [
    "# LightGBM Scratch â€“ End-to-End Benchmarks\n",
    "\n",
    "This notebook consolidates the example scripts into one place:\n",
    "\n",
    "\n",
    "\n",
    "- California Housing regression (clean)\n",
    "\n",
    "- California Housing regression with NaNs and sparse noise features\n",
    "\n",
    "- Credit-risk binary classification with imbalance handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "343e519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import sys, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    r2_score, root_mean_squared_error, mean_absolute_error, mean_pinball_loss,\n",
    "    accuracy_score, f1_score, roc_auc_score, log_loss,\n",
    "    )\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "def get_project_root() -> Path:\n",
    "    here = Path.cwd().resolve()\n",
    "    for cand in [here, *here.parents]:\n",
    "        if (cand / \"pyproject.toml\").exists() or (cand / \"src\").exists():\n",
    "            return cand\n",
    "    return here\n",
    "\n",
    "PROJECT_ROOT = get_project_root()\n",
    "SRC_ROOT = PROJECT_ROOT / \"src\"\n",
    "sys.path.insert(0, str(SRC_ROOT))\n",
    "sys.path.insert(1, str(PROJECT_ROOT))\n",
    "from lightgbm.lgbm_regressor import LGBMRegressor  # type: ignore\n",
    "from lightgbm.lgbm_classifier import LGBMClassifier  # type: ignore\n",
    "from lightgbm.loss_functions import HuberLoss, QuantileLoss\n",
    "\n",
    "\n",
    "def num_trees(model_name, model):\n",
    "    if \"GradientBoost\" in model_name:\n",
    "        return len(getattr(model, \"estimators_\", []))\n",
    "    if \"XGBoost\" in model_name:\n",
    "        bst = model.get_booster()\n",
    "        return len(bst.get_dump()) if bst is not None else getattr(model, \"n_estimators\", 0)\n",
    "    return len(getattr(model, \"trees_\", []))\n",
    "\n",
    "\n",
    "def eval_reg(model_name, model, X_train, X_test, y_train, y_test):\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    t = time.time() - start\n",
    "    y_pred = model.predict(X_test)\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"R2\": r2_score(y_test, y_pred),\n",
    "        \"RMSE\": root_mean_squared_error(y_test, y_pred),\n",
    "        \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"Training time\": t,\n",
    "        \"Num Trees Used\": num_trees(model_name, model),\n",
    "        \"y_pred\": y_pred,\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_clf(model_name, model, X_train, X_test, y_train, y_test, sample_weight=None):\n",
    "    start = time.time()\n",
    "    if sample_weight is not None:\n",
    "        try:\n",
    "            model.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        except TypeError:\n",
    "            model.fit(X_train, y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    t = time.time() - start\n",
    "    y_pred = model.predict(X_test)\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        y_proba = None\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"F1\": f1_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba if y_proba is not None else y_pred),\n",
    "        \"LogLoss\": log_loss(y_test, y_proba if y_proba is not None else y_pred),\n",
    "        \"Training time\": t,\n",
    "        \"Num Trees Used\": num_trees(model_name, model),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76417f1",
   "metadata": {},
   "source": [
    "## 1) California Housing Regression (clean)\n",
    "\n",
    "Baseline regression comparison on the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d17a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoost                R2=0.8004  RMSE=0.5114  MAE=0.3483  t=6.5003  N=200\n",
      "XGBoost Regressor            R2=0.8301  RMSE=0.4718  MAE=0.3096  t=0.1239  N=100\n",
      "LightGBM-leaf_wise_mse       R2=0.8356  RMSE=0.4641  MAE=0.3088  t=28.4178  N=200\n",
      "LightGBM-histogram_efb_goss  R2=0.8363  RMSE=0.4632  MAE=0.3116  t=17.5686  N=150\n",
      "LightGBM-huber_robust        R2=0.8350  RMSE=0.4650  MAE=0.3065  t=30.9003  N=200\n",
      "LightGBM-regularized_shallow R2=0.8135  RMSE=0.4943  MAE=0.3353  t=36.8477  N=400\n",
      "LightGBM-fast_wide           R2=0.8282  RMSE=0.4745  MAE=0.3210  t=20.7249  N=120\n",
      "LightGBM-goss_parallel       R2=0.8435  RMSE=0.4528  MAE=0.3043  t=43.1367  N=250\n",
      "LightGBM-efb_parallel        R2=0.8369  RMSE=0.4623  MAE=0.3061  t=28.7785  N=180\n",
      "LightGBM-exact_small         R2=0.8051  RMSE=0.5053  MAE=0.3404  t=12.6179  N=120\n",
      "LightGBM-quantile_p50        pinball=0.1521  coverage=0.5094  t=37.8705  N=300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Training time</th>\n",
       "      <th>Num Trees Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LightGBM-goss_parallel</td>\n",
       "      <td>0.843513</td>\n",
       "      <td>0.452838</td>\n",
       "      <td>0.304336</td>\n",
       "      <td>43.136731</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LightGBM-efb_parallel</td>\n",
       "      <td>0.836878</td>\n",
       "      <td>0.462338</td>\n",
       "      <td>0.306090</td>\n",
       "      <td>28.778517</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM-histogram_efb_goss</td>\n",
       "      <td>0.836267</td>\n",
       "      <td>0.463204</td>\n",
       "      <td>0.311637</td>\n",
       "      <td>17.568600</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM-leaf_wise_mse</td>\n",
       "      <td>0.835621</td>\n",
       "      <td>0.464116</td>\n",
       "      <td>0.308764</td>\n",
       "      <td>28.417764</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM-huber_robust</td>\n",
       "      <td>0.835003</td>\n",
       "      <td>0.464987</td>\n",
       "      <td>0.306470</td>\n",
       "      <td>30.900329</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost Regressor</td>\n",
       "      <td>0.830137</td>\n",
       "      <td>0.471794</td>\n",
       "      <td>0.309573</td>\n",
       "      <td>0.123906</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBM-fast_wide</td>\n",
       "      <td>0.828163</td>\n",
       "      <td>0.474528</td>\n",
       "      <td>0.321001</td>\n",
       "      <td>20.724862</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM-regularized_shallow</td>\n",
       "      <td>0.813543</td>\n",
       "      <td>0.494303</td>\n",
       "      <td>0.335272</td>\n",
       "      <td>36.847681</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LightGBM-exact_small</td>\n",
       "      <td>0.805142</td>\n",
       "      <td>0.505316</td>\n",
       "      <td>0.340412</td>\n",
       "      <td>12.617880</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GradientBoost</td>\n",
       "      <td>0.800445</td>\n",
       "      <td>0.511369</td>\n",
       "      <td>0.348343</td>\n",
       "      <td>6.500265</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model        R2      RMSE       MAE  Training time  \\\n",
       "7        LightGBM-goss_parallel  0.843513  0.452838  0.304336      43.136731   \n",
       "8         LightGBM-efb_parallel  0.836878  0.462338  0.306090      28.778517   \n",
       "3   LightGBM-histogram_efb_goss  0.836267  0.463204  0.311637      17.568600   \n",
       "2        LightGBM-leaf_wise_mse  0.835621  0.464116  0.308764      28.417764   \n",
       "4         LightGBM-huber_robust  0.835003  0.464987  0.306470      30.900329   \n",
       "1             XGBoost Regressor  0.830137  0.471794  0.309573       0.123906   \n",
       "6            LightGBM-fast_wide  0.828163  0.474528  0.321001      20.724862   \n",
       "5  LightGBM-regularized_shallow  0.813543  0.494303  0.335272      36.847681   \n",
       "9          LightGBM-exact_small  0.805142  0.505316  0.340412      12.617880   \n",
       "0                 GradientBoost  0.800445  0.511369  0.348343       6.500265   \n",
       "\n",
       "   Num Trees Used  \n",
       "7             250  \n",
       "8             180  \n",
       "3             150  \n",
       "2             200  \n",
       "4             200  \n",
       "1             100  \n",
       "6             120  \n",
       "5             400  \n",
       "9             120  \n",
       "0             200  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "regressors = [\n",
    "    (\"GradientBoost\", GradientBoostingRegressor(n_estimators=200, random_state=42)),\n",
    "    (\"XGBoost Regressor\", XGBRegressor()),\n",
    "    (\"LightGBM-leaf_wise_mse\", LGBMRegressor(\n",
    "        objective=\"mse\", learning_rate=0.1, num_iterations=200,\n",
    "        max_depth=6, num_leaves=31, min_data_in_leaf=20,\n",
    "        lambda_l2=0.0, lambda_l1=0.0,\n",
    "        bagging_fraction=0.8, feature_fraction=0.8,\n",
    "        use_histogram=True, max_bins=64, use_efb=False, enable_goss=False,\n",
    "    )),\n",
    "    (\"LightGBM-histogram_efb_goss\", LGBMRegressor(\n",
    "        objective=\"mse\", learning_rate=0.15, num_iterations=150,\n",
    "        max_depth=6, num_leaves=31, min_data_in_leaf=20,\n",
    "        lambda_l2=0.1, lambda_l1=0.0,\n",
    "        bagging_fraction=0.8, feature_fraction=0.8,\n",
    "        use_histogram=True, max_bins=64, use_efb=True, enable_goss=True,\n",
    "        goss_top_rate=0.2, goss_other_rate=0.1,\n",
    "    )),\n",
    "    (\"LightGBM-huber_robust\", LGBMRegressor(\n",
    "        objective=HuberLoss(delta=1.0), learning_rate=0.1, num_iterations=200,\n",
    "        max_depth=6, num_leaves=31, min_data_in_leaf=20,\n",
    "        lambda_l2=0.1, lambda_l1=0.05,\n",
    "        bagging_fraction=0.9, feature_fraction=0.9,\n",
    "        use_histogram=True, max_bins=64, use_efb=False, enable_goss=False,\n",
    "    )),\n",
    "    (\"LightGBM-regularized_shallow\", LGBMRegressor(\n",
    "        objective=\"mse\", learning_rate=0.05, num_iterations=400,\n",
    "        max_depth=4, num_leaves=15, min_data_in_leaf=30,\n",
    "        lambda_l2=1.0, lambda_l1=0.1,\n",
    "        bagging_fraction=0.7, feature_fraction=0.7,\n",
    "        use_histogram=True, max_bins=32, use_efb=False, enable_goss=False,\n",
    "    )),\n",
    "    (\"LightGBM-fast_wide\", LGBMRegressor(\n",
    "        objective=\"mse\", learning_rate=0.15, num_iterations=120,\n",
    "        max_depth=8, num_leaves=63, min_data_in_leaf=10,\n",
    "        lambda_l2=0.0, lambda_l1=0.0,\n",
    "        bagging_fraction=0.9, feature_fraction=0.9,\n",
    "        use_histogram=True, max_bins=32, use_efb=False, enable_goss=True,\n",
    "        goss_top_rate=0.2, goss_other_rate=0.1,\n",
    "    )),\n",
    "    (\"LightGBM-goss_parallel\", LGBMRegressor(\n",
    "        objective=\"mse\", learning_rate=0.08, num_iterations=250,\n",
    "        max_depth=7, num_leaves=63, min_data_in_leaf=15,\n",
    "        lambda_l2=0.2, lambda_l1=0.05,\n",
    "        bagging_fraction=0.9, feature_fraction=0.9,\n",
    "        use_histogram=True, max_bins=64, use_efb=False, enable_goss=True,\n",
    "        goss_top_rate=0.15, goss_other_rate=0.1,\n",
    "    )),\n",
    "    (\"LightGBM-efb_parallel\", LGBMRegressor(\n",
    "        objective=\"mse\", learning_rate=0.1, num_iterations=180,\n",
    "        max_depth=6, num_leaves=40, min_data_in_leaf=20,\n",
    "        lambda_l2=0.1, lambda_l1=0.0,\n",
    "        bagging_fraction=0.8, feature_fraction=0.9,\n",
    "        use_histogram=True, max_bins=64, use_efb=True, enable_goss=False,\n",
    "    )),\n",
    "    (\"LightGBM-exact_small\", LGBMRegressor(\n",
    "        objective=\"mse\", learning_rate=0.05, num_iterations=120,\n",
    "        max_depth=5, num_leaves=31, min_data_in_leaf=25,\n",
    "        lambda_l2=0.1, lambda_l1=0.05,\n",
    "        bagging_fraction=0.8, feature_fraction=0.8,\n",
    "        use_histogram=False, use_efb=False, enable_goss=False,\n",
    "    )),\n",
    "    (\"LightGBM-quantile_p50\", LGBMRegressor(\n",
    "        objective=QuantileLoss(quantile=0.5), learning_rate=0.08, num_iterations=300,\n",
    "        max_depth=5, num_leaves=25, min_data_in_leaf=20,\n",
    "        lambda_l2=0.2, lambda_l1=0.05,\n",
    "        bagging_fraction=0.8, feature_fraction=0.8,\n",
    "        use_histogram=True, max_bins=64, use_efb=False, enable_goss=False,\n",
    "    )),\n",
    "]\n",
    "\n",
    "reg_results = []\n",
    "for name, model in regressors:\n",
    "    res = eval_reg(name, model, X_train, X_test, y_train, y_test)\n",
    "    if name == \"LightGBM-quantile_p50\":\n",
    "        pb = mean_pinball_loss(y_test, res[\"y_pred\"], alpha=0.5)\n",
    "        cvg = np.mean(y_test <= res[\"y_pred\"])\n",
    "        print(f\"{name:<28} pinball={pb:.4f}  coverage={cvg:.4f}  t={res['Training time']:.4f}  N={res['Num Trees Used']}\")\n",
    "    else:\n",
    "        print(f\"{name:<28} R2={res['R2']:.4f}  RMSE={res['RMSE']:.4f}  MAE={res['MAE']:.4f}  t={res['Training time']:.4f}  N={res['Num Trees Used']}\")\n",
    "        reg_results.append({k: v for k, v in res.items() if k != \"y_pred\"})\n",
    "\n",
    "pd.DataFrame(reg_results).sort_values(by='R2', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f6405",
   "metadata": {},
   "source": [
    "## 2) Regression with NaNs and Sparse Noise\n",
    "\n",
    "Inject 30% NaNs and append 50 sparse noise features to stress EFB/GOSS and robustness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e102cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Regressor            R2=0.6496  RMSE=0.6776  MAE=0.4817  t=0.5775  N=100\n",
      "LightGBM-leaf_wise_mse       R2=0.6424  RMSE=0.6846  MAE=0.4878  t=80.2770  N=200\n",
      "LightGBM-histogram_efb_goss  R2=0.6588  RMSE=0.6686  MAE=0.4730  t=17.6634  N=150\n",
      "LightGBM-huber_robust        R2=0.6435  RMSE=0.6835  MAE=0.4741  t=90.3031  N=200\n",
      "LightGBM-regularized_shallow R2=0.6098  RMSE=0.7151  MAE=0.5131  t=91.6388  N=400\n",
      "LightGBM-fast_wide           R2=0.6079  RMSE=0.7168  MAE=0.5138  t=57.7457  N=120\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m reg_results_noise = []\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m regressors:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     res = \u001b[43meval_reg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mLightGBM-quantile_p50\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     35\u001b[39m         pb = mean_pinball_loss(y_test, res[\u001b[33m\"\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m\"\u001b[39m], alpha=\u001b[32m0.5\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36meval_reg\u001b[39m\u001b[34m(model_name, model, X_train, X_test, y_train, y_test)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meval_reg\u001b[39m(model_name, model, X_train, X_test, y_train, y_test):\n\u001b[32m     43\u001b[39m     start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     t = time.time() - start\n\u001b[32m     46\u001b[39m     y_pred = model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Bureau\\S7\\KDD\\KDD_projet\\src\\lightgbm\\lgbm_regressor.py:304\u001b[39m, in \u001b[36mLGBMRegressor.fit\u001b[39m\u001b[34m(self, X, y, eval_set, sample_weight, callbacks)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m._goss = GOSS(\n\u001b[32m    298\u001b[39m         top_rate=\u001b[38;5;28mself\u001b[39m.params.goss_top_rate,\n\u001b[32m    299\u001b[39m         other_rate=\u001b[38;5;28mself\u001b[39m.params.goss_other_rate,\n\u001b[32m    300\u001b[39m         random_state=\u001b[38;5;28mself\u001b[39m.params.random_state,\n\u001b[32m    301\u001b[39m     )\n\u001b[32m    303\u001b[39m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[38;5;28mself\u001b[39m.is_fitted_ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Bureau\\S7\\KDD\\KDD_projet\\src\\lightgbm\\lgbm_regressor.py:407\u001b[39m, in \u001b[36mLGBMRegressor._train\u001b[39m\u001b[34m(self, X, y, X_val, y_val, sample_weight)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;66;03m# Build tree\u001b[39;00m\n\u001b[32m    393\u001b[39m tree = DecisionTree(\n\u001b[32m    394\u001b[39m     max_depth=\u001b[38;5;28mself\u001b[39m.params.max_depth,\n\u001b[32m    395\u001b[39m     min_samples_leaf=\u001b[38;5;28mself\u001b[39m.params.min_data_in_leaf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    404\u001b[39m     random_state=\u001b[38;5;28mself\u001b[39m.params.random_state,\n\u001b[32m    405\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradients_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhessians_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# Update predictions\u001b[39;00m\n\u001b[32m    410\u001b[39m tree_predictions = tree.predict(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Bureau\\S7\\KDD\\KDD_projet\\src\\lightgbm\\tree.py:246\u001b[39m, in \u001b[36mDecisionTree.fit\u001b[39m\u001b[34m(self, X, gradients, hessians, sample_weight)\u001b[39m\n\u001b[32m    243\u001b[39m indices = np.arange(n_samples)\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m# Build tree using leaf-wise growth\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_tree_leaf_wise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhessians\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Bureau\\S7\\KDD\\KDD_projet\\src\\lightgbm\\tree.py:339\u001b[39m, in \u001b[36mDecisionTree._build_tree_leaf_wise\u001b[39m\u001b[34m(self, X, gradients, hessians, indices)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m child, child_indices \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    335\u001b[39m     (node.left, left_indices),\n\u001b[32m    336\u001b[39m     (node.right, right_indices),\n\u001b[32m    337\u001b[39m ]:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(child_indices) >= \u001b[32m2\u001b[39m * \u001b[38;5;28mself\u001b[39m.min_samples_leaf:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m         child_split = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_find_best_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhessians\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdepth\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m child_split.gain > \u001b[38;5;28mself\u001b[39m.min_gain_to_split:\n\u001b[32m    343\u001b[39m             heapq.heappush(\n\u001b[32m    344\u001b[39m                 split_candidates,\n\u001b[32m    345\u001b[39m                 (-child_split.gain, node_counter, child, child_indices, child_split)\n\u001b[32m    346\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Bureau\\S7\\KDD\\KDD_projet\\src\\lightgbm\\tree.py:400\u001b[39m, in \u001b[36mDecisionTree._find_best_split\u001b[39m\u001b[34m(self, X, gradients, hessians, indices, depth)\u001b[39m\n\u001b[32m    397\u001b[39m feature_values = X[indices, feature_idx]\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_histogram:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     split_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_find_best_split_histogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_gradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_hessians\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_total\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH_total\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_score\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m     split_info = \u001b[38;5;28mself\u001b[39m._find_best_split_exact(\n\u001b[32m    406\u001b[39m         feature_values, node_gradients, node_hessians,\n\u001b[32m    407\u001b[39m         indices, feature_idx, G_total, H_total, current_score\n\u001b[32m    408\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Bureau\\S7\\KDD\\KDD_projet\\src\\lightgbm\\tree.py:557\u001b[39m, in \u001b[36mDecisionTree._find_best_split_histogram\u001b[39m\u001b[34m(self, feature_values, gradients, hessians, indices, feature_idx, G_total, H_total, current_score)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valid_values) < \u001b[32m2\u001b[39m:\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best_split\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m bin_indices = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdigitize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_edges\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m bin_indices = np.clip(bin_indices, \u001b[32m0\u001b[39m, n_bins - \u001b[32m1\u001b[39m)\n\u001b[32m    560\u001b[39m \u001b[38;5;66;03m# Use numpy bincount for fast histogram on valid (non-NaN) entries\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mazbi\\anaconda3\\envs\\rl_env\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:5827\u001b[39m, in \u001b[36mdigitize\u001b[39m\u001b[34m(x, bins, right)\u001b[39m\n\u001b[32m   5825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) - _nx.searchsorted(bins[::-\u001b[32m1\u001b[39m], x, side=side)\n\u001b[32m   5826\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5827\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[43m=\u001b[49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mazbi\\anaconda3\\envs\\rl_env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:1527\u001b[39m, in \u001b[36msearchsorted\u001b[39m\u001b[34m(a, v, side, sorter)\u001b[39m\n\u001b[32m   1447\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_searchsorted_dispatcher)\n\u001b[32m   1448\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearchsorted\u001b[39m(a, v, side=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m, sorter=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1449\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1450\u001b[39m \u001b[33;03m    Find indices where elements should be inserted to maintain order.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1525\u001b[39m \u001b[33;03m    30  # The element at index 2 of the sorted array is 30.\u001b[39;00m\n\u001b[32m   1526\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msearchsorted\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[43m=\u001b[49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[43m=\u001b[49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mazbi\\anaconda3\\envs\\rl_env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# GradientBoosting can't work with NaN value, hence we only leave XGBoost and our Light GBM\n",
    "regressors = regressors[1:]\n",
    "\n",
    "def mask_nan(X, ratio=0.3, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Xc = X.copy()\n",
    "    mask = rng.random(Xc.shape) < ratio\n",
    "    Xc.values[mask] = np.nan\n",
    "    return Xc\n",
    "\n",
    "def sparse_noise(n_samples, n_features, zero_ratio=0.8, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Z = rng.standard_normal((n_samples, n_features))\n",
    "    Z[rng.random((n_samples, n_features)) < zero_ratio] = 0.0\n",
    "    return Z\n",
    "\n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = mask_nan(X_train, ratio=0.3, seed=1)\n",
    "X_test  = mask_nan(X_test,  ratio=0.3, seed=2)\n",
    "\n",
    "n_noise = 50\n",
    "noise_train = sparse_noise(len(X_train), n_noise, zero_ratio=0.8, seed=3)\n",
    "noise_test  = sparse_noise(len(X_test),  n_noise, zero_ratio=0.8, seed=4)\n",
    "noise_cols = [f\"noise_{i}\" for i in range(n_noise)]\n",
    "\n",
    "X_train_aug = pd.concat([X_train, pd.DataFrame(noise_train, columns=noise_cols, index=X_train.index)], axis=1)\n",
    "X_test_aug  = pd.concat([X_test,  pd.DataFrame(noise_test,  columns=noise_cols, index=X_test.index)], axis=1)\n",
    "\n",
    "reg_results_noise = []\n",
    "for name, model in regressors:\n",
    "    res = eval_reg(name, model, X_train_aug, X_test_aug, y_train, y_test)\n",
    "    if name == \"LightGBM-quantile_p50\":\n",
    "        pb = mean_pinball_loss(y_test, res[\"y_pred\"], alpha=0.5)\n",
    "        cvg = np.mean(y_test <= res[\"y_pred\"])\n",
    "        print(f\"{name:<28} pinball={pb:.4f}  coverage={cvg:.4f}  t={res['Training time']:.4f}  N={res['Num Trees Used']}\")\n",
    "    else:\n",
    "        print(f\"{name:<28} R2={res['R2']:.4f}  RMSE={res['RMSE']:.4f}  MAE={res['MAE']:.4f}  t={res['Training time']:.4f}  N={res['Num Trees Used']}\")\n",
    "        reg_results_noise.append({k: v for k, v in res.items() if k != \"y_pred\"})\n",
    "\n",
    "pd.DataFrame(reg_results_noise).sort_values(by='R2', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb067c",
   "metadata": {},
   "source": [
    "## 3) Credit-Risk Classification (imbalanced)\n",
    "\n",
    "Binary classification with stratified split and class weighting; compares XGBoost and several LightGBM configs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31513fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PROJECT_ROOT / \"examples\" / \"credit_risk_dataset.csv\")\n",
    "X = df.drop(columns=[\"person_income\", \"loan_status\"])\n",
    "y = df[\"loan_status\"].astype(int)\n",
    "\n",
    "cat_cols = ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "pos = (y_train == 1).sum()\n",
    "neg = (y_train == 0).sum()\n",
    "pos_weight = neg / max(pos, 1)\n",
    "sample_weight = np.where(y_train == 1, pos_weight, 1.0)\n",
    "print(f\"Class balance train: pos={pos}, neg={neg}, pos_weight={pos_weight:.3f}\")\n",
    "\n",
    "classifiers = [\n",
    "    (\"XGBoostClassifier\", XGBClassifier(\n",
    "        objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "        random_state=42, scale_pos_weight=pos_weight,\n",
    "    )),\n",
    "    (\"LightGBM-leaf_wise_binary\", LGBMClassifier(\n",
    "        objective=\"binary\", learning_rate=0.1, num_iterations=200,\n",
    "        max_depth=6, num_leaves=31, min_data_in_leaf=20,\n",
    "        lambda_l2=0.0, lambda_l1=0.0,\n",
    "        bagging_fraction=0.8, feature_fraction=0.8,\n",
    "        use_histogram=True, max_bins=64, use_efb=False, enable_goss=False,\n",
    "    )),\n",
    "    (\"LightGBM-histogram_efb_goss\", LGBMClassifier(\n",
    "        objective=\"binary\", learning_rate=0.12, num_iterations=160,\n",
    "        max_depth=6, num_leaves=31, min_data_in_leaf=20,\n",
    "        lambda_l2=0.1, lambda_l1=0.0,\n",
    "        bagging_fraction=0.8, feature_fraction=0.8,\n",
    "        use_histogram=True, max_bins=64, use_efb=True, enable_goss=True,\n",
    "        goss_top_rate=0.2, goss_other_rate=0.1,\n",
    "    )),\n",
    "    (\"LightGBM-regularized_shallow\", LGBMClassifier(\n",
    "        objective=\"binary\", learning_rate=0.05, num_iterations=300,\n",
    "        max_depth=4, num_leaves=15, min_data_in_leaf=30,\n",
    "        lambda_l2=0.5, lambda_l1=0.05,\n",
    "        bagging_fraction=0.7, feature_fraction=0.7,\n",
    "        use_histogram=True, max_bins=32, use_efb=False, enable_goss=False,\n",
    "    )),\n",
    "    (\"LightGBM-fast_wide\", LGBMClassifier(\n",
    "        objective=\"binary\", learning_rate=0.12, num_iterations=140,\n",
    "        max_depth=8, num_leaves=63, min_data_in_leaf=10,\n",
    "        lambda_l2=0.05, lambda_l1=0.0,\n",
    "        bagging_fraction=0.9, feature_fraction=0.9,\n",
    "        use_histogram=True, max_bins=32, use_efb=False, enable_goss=True,\n",
    "        goss_top_rate=0.2, goss_other_rate=0.1,\n",
    "    )),\n",
    "    (\"LightGBM-efb_parallel\", LGBMClassifier(\n",
    "        objective=\"binary\", learning_rate=0.1, num_iterations=200,\n",
    "        max_depth=6, num_leaves=40, min_data_in_leaf=20,\n",
    "        lambda_l2=0.1, lambda_l1=0.0,\n",
    "        bagging_fraction=0.8, feature_fraction=0.9,\n",
    "        use_histogram=True, max_bins=64, use_efb=True, enable_goss=False,\n",
    "    )),\n",
    "]\n",
    "\n",
    "clf_results = []\n",
    "for name, model in classifiers:\n",
    "    res = eval_clf(name, model, X_train, X_test, y_train, y_test, sample_weight)\n",
    "    print(\n",
    "        f\"{name:<28} acc={res['Accuracy']:.4f}  f1={res['F1']:.4f}  auc={res['AUC']:.4f}  logloss={res['LogLoss']:.4f}  t={res['Training time']:.4f}  N={res['Num Trees Used']}\"\n",
    "    )\n",
    "    clf_results.append(res)\n",
    "\n",
    "pd.DataFrame(clf_results).sort_values(by='Accuracy', ascending=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
