# LightGBM From Scratch

Une impl√©mentation compl√®te de LightGBM (Light Gradient Boosting Machine) en pur Python/NumPy, d√©velopp√©e comme projet acad√©mique sur les algorithmes d'arbres de d√©cision.

**üéØ Z√©ro d√©pendance sklearn** - Tous les algorithmes impl√©ment√©s from scratch avec NumPy uniquement !

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![NumPy](https://img.shields.io/badge/NumPy-Only-green)](https://numpy.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## üìã Table des mati√®res

- [Features](#-features)
- [Installation](#-installation)
- [D√©marrage rapide](#-d√©marrage-rapide)
- [Structure du projet](#-structure-du-projet)
- [Documentation](#-documentation)
- [Tests et benchmarks](#-tests-et-benchmarks)
- [Exemples d'utilisation](#-exemples-dutilisation)
- [Architecture](#Ô∏è-architecture)
- [Contributions](#-contributions)
- [Auteurs](#-auteurs)
- [Licence](#-licence)
- [R√©f√©rences](#-r√©f√©rences)

---

## üöÄ Features

### Algorithmes impl√©ment√©s

- ‚úÖ **Classification binaire & multiclasse** - Support complet des deux
- ‚úÖ **R√©gression** - MSE, MAE, Huber, Quantile loss
- ‚úÖ **Croissance leaf-wise** - Optimisation cl√© de LightGBM
- ‚úÖ **GOSS** (Gradient-based One-Side Sampling) - Entra√Ænement ~2-3x plus rapide
- ‚úÖ **Histogram Binning** - Recherche efficace de splits
- ‚úÖ **EFB** (Exclusive Feature Bundling) - Pour donn√©es haute dimension
- ‚úÖ **Early Stopping** - Pr√©vient l'overfitting
- ‚úÖ **R√©gularisation L1/L2** - Contr√¥le de la complexit√©
- ‚úÖ **Feature Subsampling** - S√©lection al√©atoire de features
- ‚úÖ **Sample Weighting** - Support des poids d'√©chantillons
- ‚úÖ **API compatible sklearn** - Interface famili√®re

### Fonctions de perte disponibles

**R√©gression** :
- `MSELoss` : Mean Squared Error (L2)
- `MAELoss` : Mean Absolute Error (L1)
- `HuberLoss` : Robuste aux outliers
- `QuantileLoss` : R√©gression quantile

**Classification** :
- `BinaryCrossEntropyLoss` : Classification binaire
- `MultiClassCrossEntropyLoss` : Classification multiclasse

---

## üì¶ Installation

### Depuis source

```bash
# Cloner le d√©p√¥t
git clone https://github.com/amineEssahraoui/KDD_projet.git
cd KDD_projet

# Installation en mode d√©veloppement
pip install -e .

# Ou installer seulement les d√©pendances
pip install -r requirements.txt
```

### D√©pendances

**Runtime** (obligatoire) :
```
numpy>=1.24.0
```

**Development** (optionnel, pour tests/benchmarks) :
```
pytest>=7.0.0
pandas>=2.0.0
scikit-learn>=1.2.0
scipy>=1.11.0
matplotlib>=3.8.0
seaborn>=0.13.0
```

---

## üéØ D√©marrage rapide

### Classification

```python
from lightgbm import LGBMClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# G√©n√©rer donn√©es
X, y = make_classification(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Entra√Æner mod√®le
clf = LGBMClassifier(num_iterations=100, learning_rate=0.1, max_depth=6)
clf.fit(X_train, y_train)

# Pr√©dictions
predictions = clf.predict(X_test)
probabilities = clf.predict_proba(X_test)

print(f"Accuracy: {(predictions == y_test).mean():.4f}")
```

### R√©gression

```python
from lightgbm import LGBMRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# G√©n√©rer donn√©es
X, y = make_regression(n_samples=1000, n_features=20, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Entra√Æner mod√®le
reg = LGBMRegressor(num_iterations=100, learning_rate=0.1, max_depth=6)
reg.fit(X_train, y_train)

# Pr√©dictions
predictions = reg.predict(X_test)
```

### Early Stopping

```python
clf = LGBMClassifier(num_iterations=1000, early_stopping_rounds=10)
clf.fit(X_train, y_train, eval_set=(X_val, y_val))
print(f"Arr√™t√© √† l'it√©ration: {clf.n_iter_}")
```

### GOSS (acc√©l√©ration)

```python
clf = LGBMClassifier(
    num_iterations=100, 
    enable_goss=True, 
    goss_top_rate=0.2
)
clf.fit(X_train, y_train)
```

---

## üìÅ Structure du projet

```
KDD_projet/
‚îÇ
‚îú‚îÄ‚îÄ src/lightgbm/              # üì¶ Package principal
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py            # Exports publics
‚îÇ   ‚îú‚îÄ‚îÄ base.py                # Classes de base (BaseEstimator, BoosterParams, Callback)
‚îÇ   ‚îú‚îÄ‚îÄ lgbm_classifier.py     # LGBMClassifier
‚îÇ   ‚îú‚îÄ‚îÄ lgbm_regressor.py      # LGBMRegressor
‚îÇ   ‚îú‚îÄ‚îÄ tree.py                # DecisionTree avec croissance leaf-wise
‚îÇ   ‚îú‚îÄ‚îÄ histogram.py           # Histogram binning (int√©gr√© dans tree.py)
‚îÇ   ‚îú‚îÄ‚îÄ goss.py                # GOSS sampling (classe GOSS, apply_goss)
‚îÇ   ‚îú‚îÄ‚îÄ efb.py                 # Exclusive Feature Bundling (FeatureBundler, bundle_features)
‚îÇ   ‚îú‚îÄ‚îÄ loss_functions.py      # Fonctions de perte + gradients/hessians
‚îÇ   ‚îî‚îÄ‚îÄ utils.py               # Validation et utilitaires
‚îÇ
‚îú‚îÄ‚îÄ tests/                     # ‚úÖ Suite de tests
‚îÇ   ‚îú‚îÄ‚îÄ test_classifier.py     # Tests LGBMClassifier
‚îÇ   ‚îú‚îÄ‚îÄ test_regressor.py      # Tests LGBMRegressor
‚îÇ   ‚îú‚îÄ‚îÄ test_tree.py           # Tests DecisionTree
‚îÇ   ‚îú‚îÄ‚îÄ test_goss.py           # Tests GOSS
‚îÇ   ‚îú‚îÄ‚îÄ test_utils.py          # Tests utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ test_math_integrity.py # Validation math√©matique
‚îÇ   ‚îî‚îÄ‚îÄ test_logic_sanity.py   # Tests de sanit√©
‚îÇ
‚îú‚îÄ‚îÄ benchmarks/                # üìä Comparaisons de performance
‚îÇ   ‚îî‚îÄ‚îÄ benchmark_comparison.py # Compare avec sklearn GradientBoosting
‚îÇ
‚îú‚îÄ‚îÄ examples/                  # üìñ Exemples d'utilisation
‚îÇ   ‚îú‚îÄ‚îÄ complete_testing.ipynb # Notebook complet avec exemples
‚îÇ   ‚îî‚îÄ‚îÄ regression_pipeline.py # Pipeline de r√©gression
‚îÇ
‚îú‚îÄ‚îÄ docs/                      # üìö Documentation
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md        # Architecture d√©taill√©e
‚îÇ   ‚îî‚îÄ‚îÄ IMPLEMENTATION_GUIDE.md # Guide d'utilisation
‚îÇ
‚îú‚îÄ‚îÄ .github/workflows/         # üîÑ CI/CD
‚îÇ   ‚îî‚îÄ‚îÄ ci.yml                 # GitHub Actions
‚îÇ
‚îú‚îÄ‚îÄ pyproject.toml             # Configuration du projet
‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances
‚îú‚îÄ‚îÄ LICENSE                    # Licence MIT
‚îî‚îÄ‚îÄ README.md                  # Ce fichier
```

---

## üìö Documentation

### Fichiers principaux

#### Package source (`src/lightgbm/`)

| Fichier | Description | Classes/Fonctions principales |
|---------|-------------|-------------------------------|
| `__init__.py` | Point d'entr√©e du package | Exports publics |
| `base.py` | Classes abstraites de base | `BaseEstimator`, `BoosterParams`, `Callback`, `EarlyStoppingCallback` |
| `lgbm_classifier.py` | Classificateur gradient boosting | `LGBMClassifier` |
| `lgbm_regressor.py` | R√©gresseur gradient boosting | `LGBMRegressor` |
| `tree.py` | Arbre de d√©cision | `DecisionTree`, `TreeNode`, `SplitInfo` |
| `loss_functions.py` | Fonctions de perte | `MSELoss`, `MAELoss`, `HuberLoss`, `QuantileLoss`, `BinaryCrossEntropyLoss`, `MultiClassCrossEntropyLoss`, `get_loss_function()` |
| `goss.py` | GOSS sampling | `GOSS`, `apply_goss()` |
| `efb.py` | Feature bundling | `FeatureBundler`, `bundle_features()` |
| `utils.py` | Utilitaires | `check_array()`, `check_X_y()`, `train_test_split()`, `accuracy_score()`, `mean_squared_error()`, etc. |

### Imports courants

```python
# Estimateurs
from lightgbm import LGBMClassifier, LGBMRegressor

# Arbres et structures
from lightgbm import DecisionTree, TreeNode, SplitInfo

# Fonctions de perte
from lightgbm.loss_functions import (
    MSELoss, MAELoss, HuberLoss, QuantileLoss,
    BinaryCrossEntropyLoss, MultiClassCrossEntropyLoss,
    get_loss_function
)

# Features avanc√©es
from lightgbm import GOSS, FeatureBundler

# Utilitaires
from lightgbm.utils import (
    train_test_split, accuracy_score, mean_squared_error,
    mean_absolute_error, r2_score
)

# Callbacks
from lightgbm.base import EarlyStoppingCallback
```

### Guides d√©taill√©s

- **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** : Architecture compl√®te du syst√®me
  - Vue d'ensemble des modules
  - Diagrammes de classes et s√©quences
  - Formules math√©matiques
  - Flux de donn√©es

- **[IMPLEMENTATION_GUIDE.md](docs/IMPLEMENTATION_GUIDE.md)** : Guide d'utilisation pratique
  - Exemples d√©taill√©s
  - Tuning des hyperparam√®tres
  - Features avanc√©es (GOSS, EFB, callbacks)
  - Troubleshooting

---

## ‚úÖ Tests et benchmarks

### Ex√©cuter les tests

```bash
# Tous les tests
python -m pytest tests/ -v

# Tests sp√©cifiques
python -m pytest tests/test_classifier.py -v
python -m pytest tests/test_regressor.py -v

# Avec couverture
python -m pytest tests/ --cov=src/lightgbm --cov-report=html
```

### Tests disponibles

| Fichier de test | Description |
|-----------------|-------------|
| `test_classifier.py` | Classification binaire et multiclasse |
| `test_regressor.py` | R√©gression avec diff√©rentes loss |
| `test_tree.py` | Arbres de d√©cision leaf-wise |
| `test_goss.py` | GOSS sampling |
| `test_utils.py` | Fonctions utilitaires |
| `test_math_integrity.py` | Validation math√©matique (gradients, hessians, gains) |
| `test_logic_sanity.py` | Tests de sanit√© (overfitting, convergence) |

### Benchmarks

```bash
# Comparer avec sklearn
python benchmarks/benchmark_comparison.py
```

**R√©sultats typiques** (n=2000, 50 arbres) :

| T√¢che | Notre LightGBM | sklearn | Rapport vitesse |
|-------|---------------|---------|-----------------|
| Classification binaire | 89.8% acc | 89.6% acc | ~2x plus lent |
| R√©gression | 90.9% R¬≤ | 89.1% R¬≤ | ~2x plus lent |
| Multiclasse (3 classes) | 95.0% acc | - | Fonctionnel ! |

Notre impl√©mentation atteint une pr√©cision comparable √† sklearn tout en √©tant seulement ~2x plus lente (Python pur vs Cython).

---

## üìñ Exemples d'utilisation

### 1. R√©gression basique

```python
from lightgbm import LGBMRegressor
import numpy as np

# Donn√©es
X = np.random.randn(1000, 10)
y = 3*X[:, 0] + 2*X[:, 1] + np.random.randn(1000)*0.5

# Mod√®le
model = LGBMRegressor(
    num_iterations=100,
    learning_rate=0.1,
    max_depth=6,
    random_state=42
)

model.fit(X, y)
predictions = model.predict(X)

print(f"MSE: {np.mean((y - predictions)**2):.4f}")
```

### 2. Classification avec validation

```python
from lightgbm import LGBMClassifier
from lightgbm.utils import train_test_split, accuracy_score
import numpy as np

# Donn√©es
X = np.random.randn(1000, 15)
y = (X[:, 0] + X[:, 1] > 0).astype(int)

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Mod√®le
clf = LGBMClassifier(num_iterations=100, random_state=42)
clf.fit(X_train, y_train)

# √âvaluation
y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.4f}")
```

### 3. Fonction de perte personnalis√©e (Huber)

```python
from lightgbm import LGBMRegressor
from lightgbm.loss_functions import HuberLoss
import numpy as np

# Donn√©es avec outliers
X = np.random.randn(500, 5)
y = X[:, 0] + 2*X[:, 1] + np.random.randn(500)*0.5
y[::50] += 10  # Ajouter outliers

# Huber loss (robuste)
model = LGBMRegressor(
    objective=HuberLoss(delta=1.0),
    num_iterations=100,
    learning_rate=0.1
)
model.fit(X, y)
```

### 4. GOSS pour grandes donn√©es

```python
from lightgbm import LGBMRegressor
import numpy as np

# Grandes donn√©es
X = np.random.randn(50000, 30)
y = X[:, 0] + 2*X[:, 1] + np.random.randn(50000)*0.5

# Avec GOSS (plus rapide)
model = LGBMRegressor(
    num_iterations=100,
    enable_goss=True,
    goss_top_rate=0.2,
    goss_other_rate=0.1,
    use_histogram=True,
    max_bins=128
)
model.fit(X, y)
```

### 5. Early stopping avec validation

```python
from lightgbm import LGBMClassifier
from lightgbm.utils import train_test_split
import numpy as np

X = np.random.randn(1000, 10)
y = (X[:, 0] + X[:, 1] > 0).astype(int)

X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, random_state=42
)

clf = LGBMClassifier(
    num_iterations=1000,
    early_stopping_rounds=20,
    learning_rate=0.1,
    verbose=1
)

clf.fit(X_train, y_train, eval_set=(X_val, y_val))
print(f"Arr√™t√© √†: {clf.n_iter_}")
```

### Plus d'exemples

Consultez le notebook `examples/complete_testing.ipynb` pour des exemples complets avec :
- R√©gression California Housing
- R√©gression avec NaN et features sparse
- Classification cr√©dit avec classes d√©s√©quilibr√©es
- Comparaisons de performances

---

## üèóÔ∏è Architecture

### Principes de conception

Notre impl√©mentation suit fid√®lement le papier LightGBM original avec ces diff√©rences cl√©s par rapport au gradient boosting standard :

1. **Croissance leaf-wise** vs level-wise
   - S√©lectionne et split la feuille avec le gain maximal
   - Plus efficace que croissance par niveau (XGBoost)

2. **GOSS** (Gradient-based One-Side Sampling)
   - Garde tous les √©chantillons avec grands gradients
   - √âchantillonne les petits gradients
   - R√©duit donn√©es de ~70% sans perte de pr√©cision

3. **EFB** (Exclusive Feature Bundling)
   - Combine features mutuellement exclusives
   - R√©duit dimensionnalit√© pour donn√©es sparse

4. **Histogram Binning**
   - Discr√©tise features continues en bins
   - Complexit√© O(max_bins) au lieu de O(n_samples)

### Formules math√©matiques cl√©s

**Gain de split** :
```
Gain = [G¬≤_L/(H_L+Œª) + G¬≤_R/(H_R+Œª) - G¬≤/(H+Œª)] / 2 - Œ≥

o√π:
  G = Œ£ gradients
  H = Œ£ hessians
  Œª = lambda_l2 (r√©gularisation L2)
  Œ≥ = min_gain_to_split
```

**Valeur de feuille optimale** :
```
w* = -G / (H + Œª)
```

**Pr√©diction finale** :
```
≈∑ = init_prediction + learning_rate √ó Œ£ tree_k(x)
```

Pour plus de d√©tails, voir [ARCHITECTURE.md](docs/ARCHITECTURE.md).

---

## üéõÔ∏è Hyperparam√®tres

| Param√®tre | Default | Description |
|-----------|---------|-------------|
| `num_iterations` / `n_estimators` | 100 | Nombre d'arbres |
| `learning_rate` | 0.1 | Taux d'apprentissage |
| `max_depth` | -1 | Profondeur max (-1 = illimit√©) |
| `num_leaves` | 31 | Nombre max de feuilles par arbre |
| `min_data_in_leaf` / `min_samples_leaf` | 20 | √âchantillons min par feuille |
| `lambda_l1` / `reg_alpha` | 0.0 | R√©gularisation L1 |
| `lambda_l2` / `reg_lambda` | 0.0 | R√©gularisation L2 |
| `feature_fraction` | 1.0 | Fraction de features par arbre |
| `bagging_fraction` | 1.0 | Fraction d'√©chantillons par arbre |
| `enable_goss` | False | Activer GOSS |
| `use_histogram` | False | Activer histogram binning |
| `early_stopping_rounds` | None | Patience pour early stopping |

Voir [IMPLEMENTATION_GUIDE.md](docs/IMPLEMENTATION_GUIDE.md) pour guide complet de tuning.

---

## ü§ù Contributions

Les contributions sont les bienvenues ! Pour contribuer :

1. Fork le d√©p√¥t
2. Cr√©er une branche feature (`git checkout -b feature/amazing-feature`)
3. Commit les changements (`git commit -m 'Add amazing feature'`)
4. Push vers la branche (`git push origin feature/amazing-feature`)
5. Ouvrir une Pull Request

### Guidelines

- Suivre le style de code existant
- Ajouter des tests pour nouvelles features
- Mettre √† jour la documentation
- S'assurer que tous les tests passent

---

## üë• Auteurs

- **Amine Essahraoui** 
- **Mohammed Amine Zbida**
- **Abderrarak Khall**

---

## üìÑ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour d√©tails.

---

## üìö R√©f√©rences

### Papiers scientifiques

1. **Ke, G., et al.** (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree." 
   *NeurIPS 2017*. 
   [Lien](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)

2. **Chen, T., & Guestrin, C.** (2016). "XGBoost: A Scalable Tree Boosting System." 
   *KDD 2016*. 
   [Lien](https://arxiv.org/abs/1603.02754)

3. **Friedman, J. H.** (2001). "Greedy function approximation: A gradient boosting machine." 
   *Annals of statistics*.

### Ressources en ligne

- [LightGBM Documentation officielle](https://lightgbm.readthedocs.io/)
- [Gradient Boosting Explained](https://explained.ai/gradient-boosting/)
- [Understanding LightGBM Parameters](https://lightgbm.readthedocs.io/en/latest/Parameters.html)

---

**Derni√®re mise √† jour** : D√©cembre 2025  
**Version** : 1.0.0  
**Status** : ‚úÖ Production Ready