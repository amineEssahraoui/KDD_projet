# ðŸŒ³ LightGBM Package - ImplÃ©mentation from Scratch

Implementation complÃ¨te de LightGBM (Light Gradient Boosting Machine) dans le cadre d'un projet acadÃ©mique sur les algorithmes d'arbres de dÃ©cision.

---

## ðŸ“ Structure du Projet

```
lightgbm_package/
â”‚
â”œâ”€â”€ lightgbm/                      # Package principal
â”œâ”€â”€ tests/                         # Tests unitaires
â”œâ”€â”€ examples/                      # Notebooks de dÃ©monstration
â”œâ”€â”€ docs/                          # Documentation technique
â”œâ”€â”€ README.md                      # Ce fichier
â”œâ”€â”€ setup.py                       # Configuration d'installation
â””â”€â”€ requirements.txt               # DÃ©pendances
```

---

## ðŸ“¦ Description des Fichiers

### **1. Package Principal : `lightgbm/`**

#### **`__init__.py`**
Fichier d'initialisation du package Python. Expose les classes principales pour faciliter l'import :
```python
from lightgbm import LGBMClassifier, LGBMRegressor
```

---

#### **`base.py`** - Classes de Base
**RÃ´le :** Contient la classe abstraite `BaseEstimator` dont hÃ©ritent tous les modÃ¨les LightGBM.

**Contenu :**
- `BaseEstimator` : Classe abstraite dÃ©finissant l'interface commune
  - Initialisation des hyperparamÃ¨tres (learning_rate, num_iterations, max_depth, etc.)
  - MÃ©thodes abstraites `fit()` et `predict()`
  - MÃ©thodes utilitaires pour initialisation des prÃ©dictions
  - Calcul des gradients et hessians

**Pourquoi :** Ã‰vite la duplication de code entre Classifier et Regressor, assure une interface cohÃ©rente.

---

#### **`lgbm_classifier.py`** - Classifier Principal
**RÃ´le :** ImplÃ©mentation du modÃ¨le de classification LightGBM.

**Contenu :**
- `LGBMClassifier` : Classe hÃ©ritant de `BaseEstimator`
  - Classification binaire et multiclasse
  - MÃ©thode `fit(X, y)` : entraÃ®nement du modÃ¨le
  - MÃ©thode `predict(X)` : prÃ©diction des classes
  - MÃ©thode `predict_proba(X)` : prÃ©diction des probabilitÃ©s
  - Gestion des loss functions appropriÃ©es (Binary/Multiclass Cross-Entropy)

**Utilisation typique :**
```python
clf = LGBMClassifier(num_iterations=100, learning_rate=0.1)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)
```

---

#### **`lgbm_regressor.py`** - Regressor Principal
**RÃ´le :** ImplÃ©mentation du modÃ¨le de rÃ©gression LightGBM.

**Contenu :**
- `LGBMRegressor` : Classe hÃ©ritant de `BaseEstimator`
  - RÃ©gression pour valeurs continues
  - MÃ©thode `fit(X, y)` : entraÃ®nement
  - MÃ©thode `predict(X)` : prÃ©diction des valeurs
  - Utilise MSE (Mean Squared Error) comme fonction de perte

**Utilisation typique :**
```python
reg = LGBMRegressor(num_iterations=100, learning_rate=0.1)
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
```

---

#### **`tree.py`** - Structure d'Arbre
**RÃ´le :** ImplÃ©mentation de l'arbre de dÃ©cision avec croissance leaf-wise.

**Contenu :**
- `Node` : Classe reprÃ©sentant un nÅ“ud de l'arbre
  - Attributs : feature_index, threshold, left, right, value, is_leaf
  - MÃ©thodes pour la navigation dans l'arbre
  
- `DecisionTree` : Classe de l'arbre de dÃ©cision
  - Construction de l'arbre selon stratÃ©gie leaf-wise
  - MÃ©thode `fit(X, gradients, hessians)` : construction de l'arbre
  - MÃ©thode `_find_best_split()` : recherche du meilleur split
  - MÃ©thode `_compute_leaf_value()` : calcul de la valeur optimale d'une feuille
  - MÃ©thode `predict(X)` : prÃ©diction via traversÃ©e de l'arbre

**ParticularitÃ© :** ImplÃ©mente la croissance **leaf-wise** (feuille par feuille) au lieu de **level-wise** (niveau par niveau), ce qui est une caractÃ©ristique clÃ© de LightGBM.

---

#### **`histogram.py`** - Gestion des Histogrammes
**RÃ´le :** Construction d'histogrammes (binning) pour accÃ©lÃ©rer la recherche de splits.

**Contenu :**
- `HistogramBuilder` : Classe pour la construction d'histogrammes
  - MÃ©thode `fit(X)` : crÃ©ation des bins pour chaque feature
  - MÃ©thode `transform(X)` : conversion des features en indices de bins
  - MÃ©thode `build_histogram()` : construction de l'histogramme pour une feature
  - MÃ©thode `compute_split_gain()` : calcul du gain pour chaque position de split

**Principe :** Au lieu d'Ã©valuer tous les points possibles pour un split (O(n)), on regroupe les valeurs en bins (par dÃ©faut 255) et on Ã©value uniquement les frontiÃ¨res de bins (O(bins)), ce qui accÃ©lÃ¨re considÃ©rablement l'entraÃ®nement.

---

#### **`goss.py`** - Gradient-based One-Side Sampling
**RÃ´le :** ImplÃ©mentation de GOSS, une technique d'Ã©chantillonnage intelligent.

**Contenu :**
- `GOSSSampler` : Classe pour l'Ã©chantillonnage GOSS
  - MÃ©thode `sample(gradients)` : sÃ©lection des Ã©chantillons importants
  - Retourne les indices sÃ©lectionnÃ©s et leurs poids

**Principe :** 
- Garde tous les Ã©chantillons avec les plus grands gradients (top_rate, ex: 20%)
- Ã‰chantillonne alÃ©atoirement parmi les petits gradients (other_rate, ex: 10%)
- PondÃ¨re les petits gradients pour compenser l'Ã©chantillonnage
- **Avantage :** RÃ©duit le nombre d'Ã©chantillons sans perdre d'information critique, accÃ©lÃ¨re l'entraÃ®nement.

---

#### **`efb.py`** - Exclusive Feature Bundling
**RÃ´le :** Regroupement de features mutuellement exclusives.

**Contenu :**
- `EFBBundler` : Classe pour le bundling de features
  - MÃ©thode `fit(X)` : identification des bundles
  - MÃ©thode `transform(X)` : transformation de X avec features fusionnÃ©es
  - MÃ©thode `_build_conflict_graph()` : construction du graphe de conflits
  - MÃ©thode `_greedy_bundling()` : algorithme de bundling glouton

**Principe :** Deux features sont "exclusives" si elles sont rarement non-nulles simultanÃ©ment (typique des features one-hot). On peut les fusionner en ajoutant un offset, rÃ©duisant ainsi la dimensionnalitÃ© sans perte d'information.

---

#### **`leaf_wise.py`** - Croissance Leaf-Wise
**RÃ´le :** ImplÃ©mentation spÃ©cifique de la stratÃ©gie de croissance leaf-wise.

**Contenu :**
- Fonctions utilitaires pour la croissance leaf-wise
- Gestion de la priority queue des feuilles
- Algorithmes de sÃ©lection de la meilleure feuille Ã  dÃ©velopper

**DiffÃ©rence avec level-wise :**
- **Level-wise** (XGBoost) : DÃ©veloppe tous les nÅ“uds d'un niveau avant de passer au suivant
- **Leaf-wise** (LightGBM) : Choisit la feuille avec le gain maximum et la dÃ©veloppe en prioritÃ©
- **Avantage :** Convergence plus rapide, arbres plus profonds mais plus efficaces

---

#### **`loss_functions.py`** - Fonctions de Perte
**RÃ´le :** DÃ©finition des fonctions de perte pour diffÃ©rents types de problÃ¨mes.

**Contenu :**
- `LossFunction` : Classe abstraite de base
  - MÃ©thode `loss()` : calcul de la perte
  - MÃ©thode `gradient()` : calcul du gradient (dÃ©rivÃ©e premiÃ¨re)
  - MÃ©thode `hessian()` : calcul du hessian (dÃ©rivÃ©e seconde)

- `MSELoss` : Mean Squared Error (rÃ©gression)
- `BinaryCrossEntropy` : Cross-entropy binaire (classification binaire)
- `MulticlassCrossEntropy` : Cross-entropy multiclasse (classification multiclasse)

**Pourquoi gradient et hessian :** LightGBM utilise l'approximation de second ordre (Taylor), d'oÃ¹ le besoin des dÃ©rivÃ©es premiÃ¨re et seconde.

---

#### **`metrics.py`** - MÃ©triques d'Ã‰valuation
**RÃ´le :** Fonctions pour Ã©valuer la performance des modÃ¨les.

**Contenu :**
- `accuracy_score()` : Exactitude pour classification
- `mse_score()` : Mean Squared Error
- `mae_score()` : Mean Absolute Error
- `logloss()` : Logarithmic Loss
- `auc_roc()` : Area Under ROC Curve

**Utilisation :** Permet d'Ã©valuer les modÃ¨les pendant et aprÃ¨s l'entraÃ®nement.

---

#### **`utils.py`** - Fonctions Utilitaires
**RÃ´le :** Fonctions auxiliaires utilisÃ©es dans tout le package.

**Contenu :**
- Validation des donnÃ©es d'entrÃ©e
- Conversion de formats
- Gestion des valeurs manquantes
- Fonctions mathÃ©matiques (sigmoid, softmax)
- Outils de logging et de verbositÃ©

---

### **2. Tests : `tests/`**

#### **`__init__.py`**
Fichier d'initialisation du module de tests.

---

#### **`test_classifier.py`**
Tests unitaires pour `LGBMClassifier` :
- Test d'entraÃ®nement sur donnÃ©es synthÃ©tiques
- Test de prÃ©diction
- Test de predict_proba
- Comparaison avec sklearn
- Tests de cas limites

---

#### **`test_regressor.py`**
Tests unitaires pour `LGBMRegressor` :
- Test d'entraÃ®nement
- Test de prÃ©diction
- Validation des performances (MSE, MAE)
- Comparaison avec sklearn

---

#### **`test_tree.py`**
Tests pour la structure d'arbre :
- Test de construction de nÅ“uds
- Test de croissance leaf-wise
- Test de find_best_split
- Test de prÃ©diction

---

#### **`test_histogram.py`**
Tests pour les histogrammes :
- Test de crÃ©ation de bins
- Test de transformation
- Test de calcul de gain
- Validation de la vitesse d'exÃ©cution

---

#### **`test_goss.py`**
Tests pour GOSS :
- Test de sampling
- Validation des poids
- VÃ©rification de la rÃ©duction d'Ã©chantillons

---

### **3. Examples : `examples/`**

#### **`classification_example.ipynb`**
Notebook de dÃ©monstration pour la classification :
- Chargement d'un dataset (Iris, Titanic)
- EntraÃ®nement du LGBMClassifier
- Ã‰valuation des performances
- Visualisation des rÃ©sultats
- Comparaison avec autres algorithmes

---

#### **`regression_example.ipynb`**
Notebook de dÃ©monstration pour la rÃ©gression :
- Chargement d'un dataset (Boston Housing, California Housing)
- EntraÃ®nement du LGBMRegressor
- Ã‰valuation (MSE, MAE, RÂ²)
- Visualisation des prÃ©dictions vs valeurs rÃ©elles

---

#### **`comparison_with_sklearn.ipynb`**
Notebook de comparaison :
- Benchmark LightGBM vs Sklearn GradientBoosting
- Comparaison de vitesse d'entraÃ®nement
- Comparaison de prÃ©cision
- Analyse des hyperparamÃ¨tres

---

### **4. Documentation : `docs/`**

#### **`lightgbm_theory.md`**
Documentation thÃ©orique complÃ¨te :
- Principe du Gradient Boosting
- Innovations de LightGBM (leaf-wise, histograms, GOSS, EFB)
- Formules mathÃ©matiques
- RÃ©fÃ©rences bibliographiques

---

#### **`api_reference.md`**
RÃ©fÃ©rence complÃ¨te de l'API :
- Liste de toutes les classes et mÃ©thodes
- ParamÃ¨tres dÃ©taillÃ©s
- Valeurs de retour
- Exemples d'utilisation

---

#### **`user_guide.md`**
Guide utilisateur :
- Installation
- Premiers pas
- Tutoriels pas Ã  pas
- Bonnes pratiques
- FAQ

---

### **5. Fichiers Racine**

#### **`README.md`**
Ce fichier - Vue d'ensemble du projet et description de tous les fichiers.

---

#### **`setup.py`**
Script d'installation du package :
- Configuration pour pip install
- MÃ©tadonnÃ©es du package (nom, version, auteur)
- DÃ©pendances
- Point d'entrÃ©e

**Utilisation :**
```bash
pip install -e .
```

---

#### **`requirements.txt`**
Liste des dÃ©pendances Python nÃ©cessaires :
```
numpy>=1.19.0
pandas>=1.1.0
scikit-learn>=0.23.0
matplotlib>=3.3.0
jupyter>=1.0.0
pytest>=6.0.0
```

**Installation :**
```bash
pip install -r requirements.txt
```

---

## ðŸš€ Installation

```bash
# Cloner le repository
git clone https://github.com/votre-repo/lightgbm_package.git
cd lightgbm_package

# Installer les dÃ©pendances
pip install -r requirements.txt

# Installer le package en mode dÃ©veloppement
pip install -e .
```

---

## ðŸ“– Utilisation Rapide

```python
from lightgbm import LGBMClassifier, LGBMRegressor

# Classification
clf = LGBMClassifier(num_iterations=100, learning_rate=0.1)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)

# RÃ©gression
reg = LGBMRegressor(num_iterations=100, learning_rate=0.1)
reg.fit(X_train, y_train)
predictions = reg.predict(X_test)
```

---

## ðŸ‘¥ Auteurs

Projet rÃ©alisÃ© dans le cadre du cours [Nom du Cours] par :
- [Nom Ã‰tudiant 1]
- [Nom Ã‰tudiant 2]
- [Nom Ã‰tudiant 3]

---

## ðŸ“š RÃ©fÃ©rences

- [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)
- [Documentation officielle LightGBM](https://lightgbm.readthedocs.io/)
- [Gradient Boosting Machines - Friedman](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)

---

## ðŸ“„ Licence

Ce projet est rÃ©alisÃ© Ã  des fins Ã©ducatives dans le cadre d'un projet acadÃ©mique.
